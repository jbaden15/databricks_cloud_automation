overview: >
  Establish all needed connections needed to integrate Databricks to an existing Redshift cluster in an existing VPC and perform JDBC queries to Redshift.

description: >
  Establish all needed connections needed to integrate Databricks to an existing Redshift cluster in an existing VPC and perform JDBC queries to Redshift.

  The following steps will be peformed to complete a Redshift integration:
  - Create S3 bucket for "UNLOAD" of query results (referred to as the "unload bucket")
  - Authenticate Databricks (driver and workers) to the "unload bucket" via IAM roles
  - Authenticate Redshift to the "Unload bucket" via IAM role
  - Establish VPC peering and necessary routing/permissions between Databricks "data plane" VPC and Redshift VPC for Redshift cluster to Spark cluster communication

use_cases: >
  - Read or write data between an existing Redshift cluster and Databricks

scope: >
  - This module assumes that the Redshift cluster already exists. It will not attempt to create the Redhshift cluster if it is not found.
  - This module assumes that the Redshift cluster already exists in a VPC.
  - This module assumes that the Redshift cluster is not configured to use a public IP address. (You must specify "not publicly accessible" when configuring).
  - This is because currently as of this writing a Redshift public IP address actually precludes a private IP address. The VPC peering submodule requires a private IP address as this is the entry it adds to the VPC route table.
  - This module does not perform Spark driver level configuration including JDBC driver installation and any necessary Databricks Redshift connectors. Please refer to docs.databricks.com/spark/latest/data-sources/aws/amazon-redshift.html#installation for guidance.